{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from torch import nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['post_id', 'comment_id', 'url', 'ancestors', 'text', 'full_context'])\n"
     ]
    }
   ],
   "source": [
    "# load training data\n",
    "training_data = pickle.load(open('../../data/reddit/bbc_news_scrape_raw.pkl', 'rb'))\n",
    "print(training_data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('msmarco-distilbert-cos-v5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 [\"it was this [raid](http://www.attorneygeneral.gov/press.aspx?id=1299)...\\numm... I don't know, I sold the rest of the weed, Paid my dude off. At first I\\nhad no idea how big it was. I just thought it was me and my roommates. The\\ngirl who's apartment I spent the night at (in her roommates empty bed) was\\nactually the girlfriend of my coke dealer (Justin in the article). She heard\\nfrom me at first, but it wasn't until day two of being on the lamb that we\\nlearned that he had been picked up for a controlled sale of a half ounce of\\ncoke to the same informant that busted me. Snitches name was Julia btw. I was\\nmore of a weed dealer, but I would do favors for friends, coke LSD Mushrooms\\netc. That snitch approached me and wanted an ounce coke. I knew I could do it.\\nI texted my dude and I was like 'I want one' he thought I wanted a gram. And\\nthat is all I gave to Julia that night. It probably ended up saving me from a\\nlot of trouble in retrospect. I had over a grand, but I couldn't go to my\\napartment, I couldn't go to my parents house. I couldn't go get my car. I\\nhonestly thought about getting a one way bus ticket. I wonder how differently\\nmy life would have turned out that day. In the search of my apartment, they\\nfound a 20 Ga sawed off double barrel. It got pinned on me. I plead out to\\nfelony delivery of cocaine, misdemeanor possession of a prohibited offensive\\nweapon. The sentence was 15 to 30 months. Did 3 in county, 8 up state, got let\\ngo early for good behavior. Even though I put a white supremacist into the\\nwall my first day in gen pop in state. He went to the ward, he needed 13\\nstitches in the back of his head. I was white, and was shaving my head by the\\ntime I hit state. I guess it sent mixed signals when I sat down to play a game\\nof chess with my black cellie. I did a week in the hole. Any way. I got out,\\nenrolled in tech school. Graduated. My last spring break before graduation, I\\nwent to the Caribbean, and ingested LSD and went snorkeling. Later that night\\nat a bar, I met a German guy. We hit it off, bsing about Hemingway and life...\\nI told him what I was doing in school. And two phone interviews later I got\\nthe job, regardless of my record. I now work in NYC. I live alone in Harlem,\\nwith a shelter dog, a former stray named Blackie Chan.\\n\\n\", \"let's make a fucking movie.\\n\\n\", \"I have often thought about writing a memoir, but the way I look at it is: My\\nstory is not finished yet. That particular chapter is behind me... but if I\\never were to document that chapter in my life. I wouldn't leave out the grimey\\ndetails. Keep in mind, you are kinda just looking at the highlights, and I\\nknow you think it sounds fun. There are a lot of funny anecdotes, and\\nexperiences... However, there are also a lot of places I have been and things\\nI been a part of, that I am not necessarily proud of... Shake downs...\\nRobberies... I have been a party to multiple home invasions. I have made\\npeople bleed in front of people they love. I have been stabbed. I have had a\\ngun to my head. I first knew I was an addict at 16, when I would get the\\nshakes. And anyone doubting the criminal element in Centre county during the\\ntime I was active can suck a dick. Our district attorney\\n[disappeared](http://en.wikipedia.org/wiki/Ray_Gricar) the year before I was\\npinched... It is still an unsolved crime. I know it is called 'Happy Valley'.\\nBut it can get grimey.\\n\\n\"] http://en.wikipedia.org/wiki/Ray_Gricar\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data), training_data[0]['full_context'], training_data[0]['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_LAST_COMMENT=False\n",
    "\n",
    "# encode the text data\n",
    "X = []\n",
    "Y = []\n",
    "for example in training_data:\n",
    "    if REMOVE_LAST_COMMENT:\n",
    "        encoded_context = sbert_model.encode(example['full_context'][:-1],convert_to_tensor=True)\n",
    "    else:\n",
    "        encoded_context = sbert_model.encode(example['full_context'],convert_to_tensor=True)\n",
    "    X.append(encoded_context)\n",
    "\n",
    "# text from the url\n",
    "Y = sbert_model.encode([example['text'] for example in training_data], convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:int(len(training_data) * 0.75)]\n",
    "Y_train = Y[:int(len(training_data) * 0.75)]\n",
    "\n",
    "X_test = X[int(len(training_data) * 0.75):]\n",
    "Y_test = Y[int(len(training_data) * 0.75):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URLSTM(\n",
       "  (lstm): LSTM(768, 768, num_layers=2)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class URLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(URLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        # self.fc = nn.Linear(hidden_size, )\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        output, state = self.lstm(x, prev_state)\n",
    "        return output, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, 1, self.input_size),\n",
    "                torch.zeros(self.num_layers, 1, self.input_size))\n",
    "\n",
    "input_size = 768\n",
    "hidden_size = 768\n",
    "num_layers = 2\n",
    "\n",
    "model = URLSTM(input_size, hidden_size, num_layers)\n",
    "model.to('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "loss_fn = nn.CosineEmbeddingLoss(margin=0.5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, Y_train, model):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for i, (x,y) in enumerate(zip(X_train, Y_train)):\n",
    "        state_h, state_c = model.init_state(len(x))\n",
    "        state_h = state_h.to('cuda:0')\n",
    "        state_c = state_c.to('cuda:0')\n",
    "\n",
    "        pred, (state_h, state_c) = model(torch.unsqueeze(x, 1), (state_h, state_c)) \n",
    "\n",
    "        condition = torch.tensor(1).to('cuda:0')\n",
    "        loss = loss_fn(state_h[-1][0], y, condition)\n",
    "\n",
    "        # randomly sample negative examples\n",
    "        # should really do contrastive loss over the batch\n",
    "        for i in range(5):\n",
    "            neg_idx = random.randint(0,len(X_train))\n",
    "            if neg_idx == i: continue\n",
    "\n",
    "            x_neg = X_train[i]\n",
    "            y_neg = Y_train[i]\n",
    "\n",
    "            state_h, state_c = model.init_state(len(x_neg))\n",
    "            state_h = state_h.to('cuda:0')\n",
    "            state_c = state_c.to('cuda:0')\n",
    "\n",
    "            pred, (state_h, state_c) = model(torch.unsqueeze(x_neg, 1), (state_h, state_c))\n",
    "\n",
    "            condition = torch.tensor(0).to('cuda:0')\n",
    "            loss_neg = loss_fn(state_h[-1][0], y_neg, condition)\n",
    "\n",
    "            loss += loss_neg\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(X_test, Y_test, model, k=10):\n",
    "    run = []\n",
    "    model.eval()\n",
    "    for i,x in enumerate(X_test):\n",
    "        state_h, state_c = model.init_state(len(x))\n",
    "        state_h = state_h.to('cuda:0')\n",
    "        state_c = state_c.to('cuda:0')\n",
    "        pred, (state_h, state_c) = model(torch.unsqueeze(x, 1), (state_h, state_c))\n",
    "\n",
    "        encoded_query = pred[-1].squeeze()\n",
    "\n",
    "        cos_scores = util.cos_sim(encoded_query, Y_test)[0]\n",
    "        top_results = torch.topk(cos_scores, k=k)\n",
    "        for j, (score, idx) in enumerate(zip(top_results[0], top_results[1])):\n",
    "            # 0 Q0 0 1 193.457108 Anserini\n",
    "            run.append(' '.join([str(i), 'Q0', str(idx.item()), str(j), str(score.item()), 'LSTM']))\n",
    "    with open('run.bbc_lstm.txt', 'w') as f:\n",
    "        for result in run:\n",
    "            f.write(result + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7720086172554228\n",
      "0.6064042072428597\n",
      "0.549935013016065\n",
      "0.5169137947691812\n",
      "0.49287047290802\n",
      "0.4731370198196835\n",
      "0.45563307891951665\n",
      "0.43929480600357057\n",
      "0.42351988543404473\n",
      "0.4079572091897329\n",
      "0.3924367056025399\n",
      "0.3769024339649412\n",
      "0.3613540124628279\n",
      "0.345854930334621\n",
      "0.3305822537740072\n",
      "0.31593244398964776\n",
      "0.3019901195367177\n",
      "0.2884418619076411\n",
      "0.27527661522229513\n",
      "0.2628413880268733\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    loss = train(X_train, Y_train, model)\n",
    "    print(loss)\n",
    "\n",
    "eval(X_test, Y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /home/mjin11/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
      "/home/mjin11/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
      "Skipping download.\n",
      "Running command: ['java', '-jar', '/home/mjin11/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-m', 'map', '-m', 'P.1', '../../data/reddit/bbc_news_rel.txt', 'run.bbc_lstm.txt']\n",
      "Results:\n",
      "map                   \tall\t0.5592\n",
      "P_1                   \tall\t0.4667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pyserini.eval.trec_eval -m map -m P.1 ../../data/reddit/bbc_news_rel.txt run.bbc_lstm.txt"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92d7a568a9f3c722ea7986f82ffeceec825677aa0986d8e5ee36e2d403eb86fb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pyserini': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

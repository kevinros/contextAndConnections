{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import urllib.parse\n",
    "from urllib.request import urlopen\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "import json\n",
    "import html2text\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for Wikipedia exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pulls the internal wiki links out of a section of text \n",
    "Returns clean text along with the link mappings\n",
    "'''\n",
    "def link_extractor(text):\n",
    "    urls = re.findall(r'href=[\\'\"]?([^\\'\" >]+)', text)\n",
    "    # wiki_cleaner encodes html or something, need to match on decoded (encoded?) brackets\n",
    "    cleaned_text = re.sub('(&lt;).*?(&gt;)', '', text)\n",
    "    return cleaned_text, [urllib.parse.unquote(url).lower() for url in urls]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of wiki_cleaner.sh\n",
    "# download wiki dump, run wiki_cleaner.sh, then run this on the output\n",
    "data_path = \"data/enwiki-20220301-pages-articles-multistream1.txt\"\n",
    "cleaned_data = {}\n",
    "with open(data_path, 'r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        json_line = json.loads(line)\n",
    "        title = json_line['title'].lower()\n",
    "        cleaned_data[title] = []\n",
    "        split_text = json_line['text'].split('\\n')\n",
    "        for text in split_text:\n",
    "            paragraph_text, urls = link_extractor(text)\n",
    "            cleaned_data[title].append({'paragraph': paragraph_text, 'links': urls})\n",
    "pickle.dump(cleaned_data, open('data/paragraphs_and_links.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "coverage is a map from each title to where it appears in the text\n",
    "    coverage['title] is list of (title, paragraph) tuples\n",
    "cleaned_data is a map from each title to its article content and outgoing links\n",
    "    cleaned_data[title] is list of {paragraph: str, links: []} tuples\n",
    "'''\n",
    "cleaned_data = pickle.load(open('data/paragraphs_and_links.pkl', 'rb'))\n",
    "coverage = {x: [] for x in cleaned_data.keys()}\n",
    "for title in cleaned_data:\n",
    "    for i,paragraph in enumerate(cleaned_data[title]):\n",
    "        for link in paragraph['links']:\n",
    "            if link in coverage:\n",
    "                coverage[link] += (title,i)\n",
    "pickle.dump(coverage, open('data/coverage.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for Reddit exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Builds comments_by_post\n",
    "'''\n",
    "# collect all comments by post\n",
    "comments_by_post = {}\n",
    "with open('../data/reddit/RC_2009-05', 'r') as f:\n",
    "    for line in f:\n",
    "        d = json.loads(line)\n",
    "        link_id = d['link_id']\n",
    "        if link_id not in comments_by_post:\n",
    "            comments_by_post[link_id] = []\n",
    "        d['body'] = html2text.html2text(d['body'])\n",
    "        comments_by_post[link_id].append(d)\n",
    "for key in list(comments_by_post.keys()):\n",
    "    if len(comments_by_post[key]) < 5:\n",
    "        del comments_by_post[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_ancestors(posts, post_id, source_id):\n",
    "    ancestors = []\n",
    "    exists_parent = True\n",
    "    while exists_parent:\n",
    "        exists_parent = False\n",
    "        \n",
    "        if post_id[:2] == 't3': # refers to a link (top-level comment)\n",
    "            # if there is a source, then we only want to return ancestors if a source is in the path\n",
    "            if source_id is not None:\n",
    "                return []\n",
    "            else:\n",
    "                return ancestors[::-1]\n",
    "\n",
    "        if post_id[:2] == 't1':\n",
    "            post_id = post_id[3:]\n",
    "\n",
    "        if post_id == source_id:\n",
    "            ancestors.append(source_id)\n",
    "            return ancestors[::-1]\n",
    "\n",
    "        for post in posts:\n",
    "            if post['id'] == post_id:\n",
    "                ancestors.append(post_id)\n",
    "                post_id = post['parent_id']\n",
    "                exists_parent=True\n",
    "    return []\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an inverted index for reddit comments\n",
    "reddit_inverted_index = {}\n",
    "for key in comments_by_post:\n",
    "    for comment in comments_by_post[key]:\n",
    "        body = comment['body']\n",
    "        body = re.sub('[^A-Za-z0-9]', ' ', body.lower()).split(' ')\n",
    "        for word in body:\n",
    "            if len(word) > 2:\n",
    "                if word not in reddit_inverted_index:\n",
    "                    reddit_inverted_index[word] = []\n",
    "                reddit_inverted_index[word].append({'post_id': key, 'comment_id': comment['id']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanatos is the ancient Greek personification of death (Mors is the Latin\n",
      "equivalent) according to Wikipedia - http://en.wikipedia.org/wiki/Thanatos\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = reddit_inverted_index['http'][7]\n",
    "for comment in comments_by_post[x['post_id']]:\n",
    "    if comment['id'] == x['comment_id']:\n",
    "        print(comment['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_possible_candidate_chains(w1, w2):\n",
    "    w1_comments = reddit_inverted_index.get(w1, [])\n",
    "    w2_comments = reddit_inverted_index.get(w2, [])\n",
    "\n",
    "    possible_candidates = []\n",
    "\n",
    "    for c1 in w1_comments:\n",
    "        for c2 in w2_comments:\n",
    "            if c1['post_id'] == c2['post_id']:\n",
    "                new_addition = {'post_id': c1['post_id'], 'source': c1['comment_id'], 'target': c2['comment_id']}\n",
    "                if new_addition not in possible_candidates:\n",
    "                    possible_candidates.append(new_addition)\n",
    "    return possible_candidates\n",
    "\n",
    "def valid_chain_check(candidate):\n",
    "    ancestors = collect_ancestors(comments_by_post[candidate['post_id']], candidate['target'], candidate['source'])\n",
    "    return ancestors\n",
    "    if candidate['source'] in ancestors:\n",
    "        return ancestors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = find_possible_candidate_chains('sackler', 'money')\n",
    "for chain in c:\n",
    "    v = valid_chain_check(chain)\n",
    "    if len(v) > 5:\n",
    "        print(chain, v)\n",
    "        for ancestor in v:\n",
    "            for post in comments_by_post[chain['post_id']]:\n",
    "                if post['id'] == ancestor:\n",
    "                    print(post['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTo make the data set:\\n\\nf(start paragraph, target paragraph) --> encoding most similar to the first paragraph of the target paragraph \\n- should really be by section?\\n\\n1. Pick a start paragraph (that is relatively well-covered)\\n2. Pick a target paragraph from one of the covered links' articles\\n3. Encode the start paragraph, the target paragraph, and the first paragraph of the target article\\n4. Train model to minimize the difference of f(.,.) with the first paragraph encoding\\n\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "To make the data set:\n",
    "\n",
    "three paragraphs start, target, hidden\n",
    "f(start, target) --> encoding most similar to hidden\n",
    "\n",
    "f(start paragraph, target paragraph) --> encoding most similar to the paragraphs above the target paragraph \n",
    "- should really be by section?\n",
    "\n",
    "1. Pick a start paragraph (that is relatively well-covered)\n",
    "2. Pick a target paragraph from one of the covered links' articles\n",
    "3. Encode the start paragraph, the target paragraph, and a paragraph right above the target paragraph in the same article\n",
    "4. Train model to minimize the difference of f(.,.) with the above-target paragraph encoding\n",
    "\n",
    "Maybe first look at basic measures\n",
    "1. Make progress towards target (controlled by threshold)\n",
    "2. Minimize introduction of unnecessary information (compactness)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "bert_model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-cased\")\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = [x['paragraph'] for title in cleaned_data for x in cleaned_data[title] if x['paragraph'] != ''][:10000]\n",
    "candidate_encodings = sbert_model.encode(candidates, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basic paragraph interpolation does not seem to work. Suspect that the distance is too vast. Motivates a stronger constraint on individual words\n",
    "\"\"\"\n",
    "\n",
    "START_IDX = 0\n",
    "END_IDX = 100 \n",
    "CHAIN_LENGTH = 10\n",
    "\n",
    "for i in range(CHAIN_LENGTH):\n",
    "    interpolated = candidate_encodings[START_IDX] * (5-i) + candidate_encodings[END_IDX] * i\n",
    "    cosine_scores = util.cos_sim(interpolated, candidate_encodings)\n",
    "    max_idx = torch.topk(cosine_scores, 2)[1][0][1]\n",
    "    print(candidates[max_idx])\n",
    "print(candidates[END_IDX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Can try looking for the most similar paragraphs in the direction of the target\n",
    "E.g: source --> n most similar --> choose 1 that is most similar to target --> repeat\n",
    "'''\n",
    "START_IDX = 0\n",
    "END_IDX = 100\n",
    "cur_source = candidate_encodings[START_IDX]\n",
    "target = candidate_encodings[END_IDX]\n",
    "print(candidates[START_IDX])\n",
    "for i in range(2):\n",
    "    source_scores = util.cos_sim(cur_source, candidate_encodings)\n",
    "    max_idx = torch.squeeze(torch.topk(source_scores, 10)[1], 0)\n",
    "    #print(max_idx)\n",
    "\n",
    "    max_candidates = torch.index_select(candidate_encodings, 0, max_idx)\n",
    "    target_scores = util.cos_sim(target, max_candidates)\n",
    "\n",
    "    max_idx_target = torch.squeeze(torch.topk(target_scores, 1)[1],0)\n",
    "    #print(max_idx_target)\n",
    "\n",
    "    print(candidates[max_idx[max_idx_target]])\n",
    "    cur_source = candidate_encodings[max_idx[max_idx_target]]\n",
    "print(candidates[END_IDX])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for Reddit URL exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_urls = []\n",
    "keywords = ['cnn', 'fox', 'nytimes']\n",
    "\n",
    "for key in comments_by_post:\n",
    "    for post in comments_by_post[key]:\n",
    "        # first check if post body contains URL\n",
    "        urls = re.findall(r'(https?://\\S+)', post['body'])\n",
    "        for url in urls:\n",
    "            for keyword in keywords:\n",
    "                if keyword in url:\n",
    "                    all_urls.append({'post_id': key, 'comment_id': post['id'], 'url': url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(url):\n",
    "    try:\n",
    "        html = urlopen(url).read()\n",
    "    except:\n",
    "        return False\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'post_id': 't3_8guzj', 'comment_id': 'c098njp', 'url': 'http://select.nytimes.com/gst/abstract.html?res=F00A11FE345C177B93C0A8178AD85F4D8485F9'}\n",
      "[{'author': '[deleted]', 'ups': -1, 'subreddit': 'worldnews', 'downs': 0, 'gilded': 0, 'score_hidden': False, 'author_flair_css_class': None, 'distinguished': None, 'author_flair_text': None, 'body': '\\\\+\\nhttp://pqasb.pqarchiver.com/latimes/access/521670312.html?dids=521670312:521670312&FMT=CITE&FMTS=CITE:AI&date=May+20%2C+1967&author=&pub=Los+Angeles+Times&desc=Israel+Warns+on+U.N.+Pullout&pqatl=google\\n\\\\+\\nhttp://select.nytimes.com/gst/abstract.html?res=F00A11FE345C177B93C0A8178AD85F4D8485F9\\n\\\\+\\nhttp://select.nytimes.com/gst/abstract.html?res=F60F13FE355D13728DDDAC0894D0405B8889F1D3\\n\\\\+\\nhttp://pqasb.pqarchiver.com/chicagotribune/access/508111992.html?dids=508111992:508111992&FMT=CITE&FMTS=CITE:AI&date=Apr+07%2C+1955&author=&pub=Chicago+Tribune&desc=ISRAEL+WARNS+U.+N.+ARMISTICE+MAY+BE+BROKEN&pqatl=google\\n\\\\+\\nhttp://select.nytimes.com/gst/abstract.html?res=FB0E11FE345F177B93CBA91789D95F4D8485F9\\n\\\\+\\nhttp://pqasb.pqarchiver.com/latimes/access/642440142.html?dids=642440142:642440142&FMT=CITE&FMTS=CITE:AI&date=Dec+08%2C+1976&author=&pub=Los+Angeles+Times&desc=Israel+Warns+It+Could+Withdraw+U.+N.+Resolution&pqatl=google\\n\\\\+ http://www.independent.co.uk/news/world/middle-east/israel-warns-britain-\\nover-un-barrier-resolution-554028.html \\\\+\\nhttp://select.nytimes.com/gst/abstract.html?res=F10B11FD395F167B93C4A8178DD85F4C8485F9\\n\\\\+\\nhttp://select.nytimes.com/gst/abstract.html?res=FA0611FE3859107B93C3A91789D95F468785F9\\n\\\\+\\nhttp://news.google.com/newspapers?id=WuUNAAAAIBAJ&sjid=i20DAAAAIBAJ&pg=7175,543153&dq=israel-\\nwarns \\\\+\\nhttp://pqasb.pqarchiver.com/chicagotribune/access/506693062.html?dids=506693062:506693062&FMT=CITE&FMTS=CITE:AI&date=Sep+30%2C+1955&author=&pub=Chicago+Tribune&desc=ISRAEL+WARNS+RUSS+OF+ARMS+DEAL+DANGERS&pqatl=google\\n\\\\+\\nhttp://pqasb.pqarchiver.com/jpost/access/937550511.html?dids=937550511:937550511&FMT=ABS&FMTS=ABS:FT&date=Nov+27%2C+2005&author=TOVAH+LAZAROFF&pub=Jerusalem+Post&desc=Israel+warns+EU+report+may+damage+relations.+%3B+Draft+policy+statement+calls+post-%2767+neighborhoods%3B+illegal+settlements%27&pqatl=google\\n\\\\+ http://www.accessmylibrary.com/coms2/summary_0286-25291956_ITM\\n\\n', 'archived': True, 'parent_id': 't1_c098hr3', 'score': -1, 'id': 'c098njp', 'retrieved_on': 1425962687, 'created_utc': '1241136001', 'name': 't1_c098njp', 'controversiality': 0, 'edited': False, 'link_id': 't3_8guzj', 'subreddit_id': 't5_2qh13'}, {'retrieved_on': 1425962688, 'created_utc': '1241136422', 'id': 'c098nq7', 'archived': True, 'parent_id': 't1_c098kgv', 'score': 5, 'distinguished': None, 'body': 'Leaving only the Americans as agents for peace? *shudders*\\n\\n', 'author_flair_text': None, 'link_id': 't3_8guzj', 'subreddit_id': 't5_2qh13', 'edited': False, 'controversiality': 0, 'name': 't1_c098nq7', 'downs': 0, 'score_hidden': False, 'gilded': 0, 'ups': 5, 'author': 'glengyron', 'subreddit': 'worldnews', 'author_flair_css_class': None}, {'created_utc': '1241136779', 'retrieved_on': 1425962691, 'id': 'c098nun', 'parent_id': 't3_8guzj', 'archived': True, 'score': 8, 'distinguished': None, 'author_flair_text': None, 'body': \"If you say mean things about us, we're not going to let you guide our peace\\ntalks? That's really a position?\\n\\n\", 'link_id': 't3_8guzj', 'subreddit_id': 't5_2qh13', 'edited': False, 'controversiality': 0, 'name': 't1_c098nun', 'downs': 0, 'score_hidden': False, 'gilded': 0, 'ups': 8, 'author': 'iltat', 'subreddit': 'worldnews', 'author_flair_css_class': None}, {'gilded': 0, 'score_hidden': False, 'downs': 0, 'subreddit': 'worldnews', 'ups': 0, 'author': 'glengyron', 'author_flair_css_class': None, 'id': 'c098nwb', 'created_utc': '1241136891', 'retrieved_on': 1425962691, 'distinguished': None, 'author_flair_text': None, 'body': \"Or Russian ones. Since far more seriously [Russia has recently warned the EU\\nabout the consequences of protests against the sovereignty of\\nMoldova](http://www.straitstimes.com/Breaking%2BNews/World/Story/STIStory_361672.html).\\nCountries warn each other all the time, it's part of the wonderful world of\\ndiplomacy. \\\\- [Britain among 5 nations warned by EU to cut budget\\ndeficits](http://www.smartbrief.com/news/cfa/storyDetails.jsp?issueid=A2F6A68E-F177-42F8-9BF5-2CB7E98B115A&copyid=E476B3A7-33D8-4921-AAD9-634645B61BF6)\\n\\\\- [EU warns UK over Internet Privacy -\\nPhorm.](http://www.ukcheapbroadband.com/broadband-blog/17234/eu-warns-uk-over-\\ninternet-privacy-phorm/) \\\\- [EU warns China over increasing steel\\nexports](http://www.telegraph.co.uk/finance/newsbysector/industry/5127401/EU-\\nwarns-China-over-increasing-steel-exports.html) \\\\- [EU warns Germany over\\ntransparency](http://www.europeanvoice.com/article/2009/04/eu-warns-germany-\\nover-transparency/64708.aspx) Reddit just loves to freak out.\\n\\n\", 'score': 0, 'archived': True, 'parent_id': 't1_c098lt9', 'edited': False, 'subreddit_id': 't5_2qh13', 'link_id': 't3_8guzj', 'name': 't1_c098nwb', 'controversiality': 0}, {'link_id': 't3_8guzj', 'subreddit_id': 't5_2qh13', 'edited': False, 'controversiality': 0, 'name': 't1_c098rta', 'created_utc': '1241147392', 'retrieved_on': 1425962742, 'id': 'c098rta', 'parent_id': 't1_c098kgv', 'archived': True, 'score': 8, 'author_flair_text': None, 'distinguished': None, 'body': 'I imagine the middle east would explode violently without all that Israeli\\npeace falling from the sky?\\n\\n', 'author_flair_css_class': None, 'downs': 0, 'gilded': 0, 'score_hidden': False, 'ups': 8, 'author': 'te_anau', 'subreddit': 'worldnews'}, {'created_utc': '1241152637', 'retrieved_on': 1425962766, 'id': 'c098tpa', 'score': 1, 'archived': True, 'parent_id': 't3_8guzj', 'author_flair_text': None, 'distinguished': None, 'body': 'O\\'really ? Or , is this a rather sad case of \"Bibi\" trying to pass much wind\\nand rhetoric in the direction of Turkey and Europe but some smart ass just\\nstuck a cork in it and it is all blocked up and constipated now !\\n\\n', 'subreddit_id': 't5_2qh13', 'link_id': 't3_8guzj', 'edited': False, 'controversiality': 0, 'name': 't1_c098tpa', 'gilded': 0, 'score_hidden': False, 'downs': 0, 'subreddit': 'worldnews', 'ups': 1, 'author': 'heystoopid', 'author_flair_css_class': None}, {'author_flair_css_class': None, 'ups': 1, 'author': 'malcontent', 'subreddit': 'worldnews', 'downs': 0, 'gilded': 0, 'score_hidden': False, 'controversiality': 0, 'name': 't1_c098vzn', 'link_id': 't3_8guzj', 'subreddit_id': 't5_2qh13', 'edited': False, 'archived': True, 'parent_id': 't1_c098hr3', 'score': 1, 'distinguished': None, 'body': \"Russia, china, iran, sudan, israel. That's about right.\\n\\n\", 'author_flair_text': None, 'created_utc': '1241160087', 'retrieved_on': 1425962805, 'id': 'c098vzn'}, {'author_flair_css_class': None, 'downs': 0, 'score_hidden': False, 'gilded': 0, 'ups': 3, 'author': 'masklinn', 'subreddit': 'worldnews', 'edited': True, 'link_id': 't3_8guzj', 'subreddit_id': 't5_2qh13', 'name': 't1_c098y7o', 'controversiality': 0, 'id': 'c098y7o', 'created_utc': '1241172307', 'retrieved_on': 1425962825, 'body': 'Your links 2, 3 and 5 are not really part of \"the wonderful part of diplomacy\"\\nany more than the fed rebuking states because they\\'re being morons would be.\\n\\n', 'distinguished': None, 'author_flair_text': None, 'parent_id': 't1_c098nwb', 'archived': True, 'score': 3}, {'retrieved_on': 1425962833, 'created_utc': '1241176480', 'id': 'c098yvk', 'score': -1, 'archived': True, 'parent_id': 't1_c098y7o', 'distinguished': None, 'body': \"I'll concede that. The language stretches across a wide variety of situations.\\n\\n\", 'author_flair_text': None, 'subreddit_id': 't5_2qh13', 'link_id': 't3_8guzj', 'edited': False, 'controversiality': 0, 'name': 't1_c098yvk', 'gilded': 0, 'score_hidden': False, 'downs': 0, 'subreddit': 'worldnews', 'author': 'glengyron', 'ups': -1, 'author_flair_css_class': None}, {'name': 't1_c0991fd', 'controversiality': 0, 'edited': False, 'subreddit_id': 't5_2qh13', 'link_id': 't3_8guzj', 'body': '[deleted]\\n\\n', 'distinguished': None, 'author_flair_text': None, 'score': 2, 'parent_id': 't3_8guzj', 'archived': True, 'id': 'c0991fd', 'created_utc': '1241185736', 'retrieved_on': 1425962880, 'author_flair_css_class': None, 'subreddit': 'worldnews', 'ups': 2, 'author': '[deleted]', 'score_hidden': False, 'gilded': 0, 'downs': 0}, {'downs': 0, 'gilded': 0, 'score_hidden': False, 'author': '[deleted]', 'ups': 0, 'subreddit': 'worldnews', 'author_flair_css_class': None, 'retrieved_on': 1425962885, 'created_utc': '1241187277', 'id': 'c099224', 'parent_id': 't1_c0991fd', 'archived': True, 'score': 0, 'distinguished': None, 'author_flair_text': None, 'body': '[deleted]\\n\\n', 'link_id': 't3_8guzj', 'subreddit_id': 't5_2qh13', 'edited': False, 'controversiality': 0, 'name': 't1_c099224'}, {'id': 'c099qxd', 'author_flair_css_class': None, 'retrieved_on': 1425963240, 'subreddit_id': 't5_2qh13', 'ups': 1, 'downs': 0, 'author_flair_text': None, 'distinguished': None, 'created_utc': '1241239166', 'score_hidden': False, 'name': 't1_c099qxd', 'link_id': 't3_8guzj', 'archived': True, 'gilded': 0, 'parent_id': 't1_c098nun', 'author': 'rcglinsk', 'controversiality': 0, 'score': 1, 'subreddit': 'worldnews', 'edited': False, 'body': 'Yes. And, they actually expect the Europeans to care.\\n\\n'}]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "load reddit comment data\n",
    "loop through comments\n",
    "    if url,\n",
    "        get ancestor pointers\n",
    "\n",
    "for all url blocks\n",
    "    extract text from html\n",
    "    \n",
    "train retrieval cross encoder model to predict likelihood of a URL being posted in a comment (or the next comment)\n",
    "    input: comment chain, URL text + metadata (domain, title)\n",
    "    output: [0-1] match\n",
    "\n",
    "can model be used to build big semantic graph? \"where to go next\"\n",
    "        \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cur = all_urls[0]\n",
    "print(cur)\n",
    "print(comments_by_post[cur['post_id']])\n",
    "\n",
    "#scrapped_text = scrape(cur['url'])\n",
    "#print(scrapped_text)\n",
    "\n",
    "ancestors = collect_ancestors(comments_by_post[cur['post_id']], cur['comment_id'], None)\n",
    "print(ancestors)\n",
    "\n",
    "for ancestor in ancestors:\n",
    "        for post in comments_by_post[cur['post_id']]:\n",
    "                    if cur['comment_id'] == ancestor:\n",
    "                        print(post['body'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

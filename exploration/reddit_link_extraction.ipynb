{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import html2text\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import pickle\n",
    "import signal\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Collect all comments by the post\n",
    "Only keep posts with more than 5 comments\n",
    "Takes about 3 minutes to run on timan107\n",
    "\n",
    "To get the reddit data (chosen arbitrarily), go to the data dir and run\n",
    "\n",
    "mkdir reddit\n",
    "cd reddit\n",
    "wget https://files.pushshift.io/reddit/comments/RC_2009-05.bz2\n",
    "bzip2 -d RC_2009-05.bz2\n",
    "'''\n",
    "\n",
    "comments_by_post = {}\n",
    "with open('../data/reddit/RC_2009-05', 'r') as f:\n",
    "    for line in f:\n",
    "        d = json.loads(line)\n",
    "        link_id = d['link_id']\n",
    "        if link_id not in comments_by_post:\n",
    "            comments_by_post[link_id] = {}\n",
    "        d['body'] = html2text.html2text(d['body'])\n",
    "        comments_by_post[link_id][d['id']] = d\n",
    "        \n",
    "for key in list(comments_by_post.keys()):\n",
    "    if len(comments_by_post[key]) < 5:\n",
    "        del comments_by_post[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_ancestors(comments: dict, comment_id: str) -> list:\n",
    "    '''\n",
    "    For a given list of comments and a comment in the list, reconstruct a path to the top-level comment\n",
    "    Returns a list of comment IDs\n",
    "\n",
    "    This method isn't very efficient but good enough for now\n",
    "    '''\n",
    "    ancestors = []\n",
    "    while True:\n",
    "        \n",
    "        if comment_id[:2] == 't3': \n",
    "            # refers to a link (top-level comment)\n",
    "            # means we've reached the top of the chain\n",
    "            return ancestors[::-1]\n",
    "\n",
    "        if comment_id[:2] == 't1':\n",
    "            comment_id = comment_id[3:]\n",
    "\n",
    "        try:\n",
    "            # there is an error here sometimes where the comment id is not present in the list\n",
    "            # probably fine for now, but may need to address in the future\n",
    "            old_comment_id = comment_id\n",
    "            comment_id = comments[comment_id]['parent_id']\n",
    "            ancestors.append(old_comment_id)\n",
    "        except:\n",
    "            return ancestors[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Cycle through all posts and comments, find any URL mentions, and save the mention location + the comment's ancestors\n",
    "'''\n",
    "all_urls = []\n",
    "for post_id in comments_by_post:\n",
    "    for comment_id in comments_by_post[post_id]:\n",
    "        \n",
    "        # first check if post body contains URL\n",
    "#         urls = re.findall(r'(https?://\\S+)', comments_by_post[post_id][comment_id]['body'])\n",
    "#         len1 = len(urls)\n",
    "\n",
    "        # check if post body contains URL, accounts for edge case when dash is at the end of the line\n",
    "        current_comment_text = comments_by_post[post_id][comment_id]['body']\n",
    "        urls = re.findall(r'(https?://\\S+-\\n)?(?(1)([\\S]*)|(https?://\\S+))', current_comment_text)\n",
    "        \n",
    "#         len2 = len(urls)\n",
    "        \n",
    "        #if len1 != len2:\n",
    "        #    print(current_comment_text)\n",
    "                \n",
    "        if urls:\n",
    "            ancestors = collect_ancestors(comments_by_post[post_id], comment_id)\n",
    "            \n",
    "            for url in urls:\n",
    "                url = \"\".join(list(url))\n",
    "\n",
    "                # heuristics for parsing errors\n",
    "                url = re.sub('\\)', '', url)\n",
    "                url = re.sub('\\]', '', url)\n",
    "                url = re.sub('\\n', '', url)\n",
    "                \n",
    "                # remove non-alphnumeric characters\n",
    "                url_letters = re.sub('[^0-9a-zA-Z]', '', url)\n",
    "                                \n",
    "                # ignore pdfs\n",
    "                if 'pdf' == url_letters[-3:] or 'jpg' in url_letters[-3:] or 'png' in url_letters[-3:] or 'gif' in url_letters[-3:]:\n",
    "                    continue\n",
    "                \n",
    "                all_urls.append({'post_id': post_id, 'comment_id': comment_id, 'url': url, 'ancestors': ancestors})\n",
    "\n",
    "urls_with_context = [x for x in all_urls if len(x['ancestors']) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loops through all ancestors of a URL comment and returns the chain up to to the top comment\n",
    "'''\n",
    "\n",
    "def get_context(url_obj: dict) -> list:\n",
    "    post_id = url_obj['post_id']\n",
    "    context = []\n",
    "    for ancestor in url_obj['ancestors']:\n",
    "        context.append(comments_by_post[post_id][ancestor]['body'])\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Simple method to scrape text from URLs. Not very robust. Need to handle exceptions. YouTube links take very long\n",
    "'''\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def handler(signum, frame):\n",
    "    # print(\"Time Exceeded!\")\n",
    "    raise TimeoutException\n",
    "\n",
    "def scrape(url: str) -> str:\n",
    "    try:\n",
    "        html = urlopen(url).read()\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "\n",
    "    alphabet_checker = re.compile('[a-zA-z]')\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        paragraph_text = paragraph.get_text()\n",
    "        if alphabet_checker.findall(paragraph_text):\n",
    "            if 'wikipedia.org' in url:\n",
    "                return paragraph_text\n",
    "            # print(paragraph.get_text())\n",
    "\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines and short lines\n",
    "    text = \"\"\n",
    "    for chunk in chunks:\n",
    "        if chunk and len(chunk) > 50:\n",
    "            # inspired by https://bigscience.huggingface.co/blog/building-a-tb-scale-multilingual-dataset-for-language-modeling#:~:text=Filters%2C%20tools%2C%20and%20indicators%20of%20data%20quality\n",
    "            only_text = re.sub('[^ \\w\\*]', '', chunk)\n",
    "            if len(only_text) / len(line) > 0.8:\n",
    "                text += chunk\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In virology, influenza A virus subtype H1N1 (A/H1N1) is a subtype of Influenza A virus. Well known outbreaks of H1N1 strains in humans include the Spanish flu, the 1977 Russian flu pandemic and the 2009 swine flu pandemic. It is an orthomyxovirus that contains the glycoproteins hemagglutinin and neuraminidase. For this reason, they are described as H1N1, H1N2 etc., depending on the type of H or N antigens they express with metabolic synergy. Hemagglutinin causes red blood cells to clump together and binds the virus to the infected cell. Neuraminidase is a type of glycoside hydrolase enzyme which helps to move the virus particles through the infected cell and assist in budding from the host cells.[1]\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape(\"http://en.wikipedia.org/wiki/H1N1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.enamerique.com[(2(http://www.enamerique.net\n",
      "[('browsehappy.com', 1), ('itiz.in', 1), ('altavista.com', 1), ('www.yahoo.com', 1), ('www.thesidebar.org', 1), ('www.pickntools.com', 1), ('ni4d.us', 1), ('nemesis.thewavelength.net', 1), ('www.northernsun.com', 1), ('www.schoolphysics.co.uk', 1), ('lost-found.se', 1), ('netradio.dr.dk', 1), ('www.classicfm.co.uk', 1), ('www.cyriak.co.uk', 1), ('manga.clone-army.org', 1), ('cbc.ca', 1), ('usgovinfo.about.com', 1), ('www.partyben.com', 1), ('data.bls.gov', 1), ('www.salem-news.com', 1), ('z.about.com', 1), ('svetlana14s.narod.ru', 1), ('www.clubsnap.com', 1), ('www.economistsubscriptions.com', 1), ('www.economistacademic.com', 1), ('bastiat.org', 1), ('www.solwise.co.uk', 1), ('www.scibooks.org', 1), ('www.crcbermuda.com', 1), ('bushwells.wordpress.com', 1), ('www.stiffs.com', 1), ('www.famous-people-', 1), ('realcostofprisons.org', 1), ('moultano.blogspot.com', 1), ('lucumr.pocoo.org', 1), ('www.houseofnumbers.com', 1), ('www.legalmatch.com', 1), ('www.flexyourrights.org', 1), ('www.cato-at-liberty.org', 1), ('www.imagecomics.com', 1), ('www.gleamingedge.com', 1), ('www.urbanterror.net,', 1), ('www.cooks.com', 1), ('www.theavenueatwhitemarsh.com', 1), ('devolab.msu.edu', 1), ('newtonsbinomium.blogspot.com', 1), ('oglaf.com', 1), ('www.platinumgrit.com', 1), ('www.gmgplc.co.uk', 1), ('www.autotrader.co.uk', 1), ('www.marcellosendos.ch', 1), ('www.tate.org.uk', 1), ('www.youtube**mp4**.com', 1), ('www.youtubemp4.com', 1), ('www.fao.org', 1), ('www.tshaonline.org', 1), ('www.lyricsdomain.com', 1), ('www.awe.co.uk', 1), ('www61.wolframalpha.com', 1), ('www.indiantrust.com', 1), ('www.aimovement.org', 1), ('www.tresemme.com', 1), ('www.folica.com', 1), ('shootout.alioth.debian.org,', 1), ('perldoc.perl.org', 1), ('cufp.galois.com', 1), ('alioth.debian.org', 1), ('www.wotug.org', 1), ('www.peshitta.org', 1), ('www.historyguy.com', 1), ('www.bra.se', 1), ('mobilitysite.com', 1), ('pressherald.mainetoday.com', 1), ('franklinchurchofchrist.com', 1), ('fallout3nexus.com', 1), ('donklephant.com', 1), ('www.woodyguthrie.org', 1), ('thefiresidepost.com', 1), ('www.koreatimes.co.kr', 1), ('www.military-today.com', 1), ('gc.kls2.com', 1), ('starcraft.wikia.com', 1), ('www.opensourcetheology.net', 1), ('www.congress.org', 1), ('www.catholicdemocrats.org', 1), ('www.christiananswers.net', 1), ('www.jensonusa.com', 1), ('www.airbomb.com', 1), ('arresteddevelopment.msn.com', 1), ('prustice.wordpress.com', 1), ('www.icompositions.com', 1), ('www.nbcnewyork.com', 1), ('bitten.edgewall.org', 1), ('economistsview.typepad.com', 1), ('www.fungi.com', 1), ('therumicworld.com', 1), ('www.darpa.mil', 1), ('www.vancouver2010.com', 1), ('www.kake.com', 1), ('www.education.com', 1)]\n"
     ]
    }
   ],
   "source": [
    "# explore distribution of domains (help us with parsing)\n",
    "domains = {}\n",
    "for url in urls_with_context:\n",
    "    try:\n",
    "        domain = urlparse(url['url']).netloc\n",
    "    except:\n",
    "        print(url['url'])\n",
    "    if domain not in domains:\n",
    "        domains[domain] = 0\n",
    "    domains[domain] += 1\n",
    "\n",
    "sorted_domains = sorted([(domain, domains[domain]) for domain in domains], reverse=True, key=lambda x: x[1])\n",
    "print(sorted_domains[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "45\n",
      "106\n",
      "138\n",
      "181\n",
      "210\n",
      "247\n",
      "288\n",
      "327\n",
      "365\n",
      "404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435\n",
      "478\n",
      "512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581\n",
      "612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652\n",
      "688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717\n",
      "752\n",
      "790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820\n",
      "861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "897\n",
      "928\n",
      "964\n",
      "1006\n",
      "1044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjin11/miniconda3/envs/mjin/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166\n",
      "1194\n",
      "1240\n",
      "1278\n",
      "1303\n",
      "1341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjin11/miniconda3/envs/mjin/lib/python3.9/site-packages/charset_normalizer/api.py:105: UserWarning: Trying to detect encoding from a tiny portion of (9) byte(s).\n",
      "  warn('Trying to detect encoding from a tiny portion of ({}) byte(s).'.format(length))\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1371\n",
      "1413\n",
      "1449\n",
      "1488\n",
      "1521\n",
      "1552\n",
      "1590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1662\n",
      "1702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740\n",
      "1776\n",
      "1809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1849\n",
      "1885\n",
      "1911\n",
      "1940\n",
      "1986\n",
      "2029\n",
      "2052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2163\n",
      "2196\n",
      "2235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.datejesus.com/ TimeoutException\n",
      "2336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2368\n",
      "2408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2481\n",
      "2514\n",
      "2548\n",
      "http://www.thestranger.com/seattle/Content?oid=30811 TimeoutException\n",
      "2583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2663\n",
      "2699\n",
      "2740\n",
      "2776\n",
      "2805\n",
      "2839\n",
      "2877\n",
      "2915\n",
      "2950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3079\n",
      "3126\n",
      "3159\n",
      "3207\n",
      "3246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3277\n",
      "3319\n",
      "http://www.visual-memory.co.uk/amk/doc/0055.html TimeoutException\n",
      "3356\n",
      "http://www.sphinxsearch.com/docs/current.html#conf-sql-query TimeoutException\n",
      "3385\n",
      "3426\n",
      "3463\n",
      "http://rkba.org/research/cramer/shall-issue.html TimeoutException\n",
      "3502\n",
      "3548\n",
      "3593\n",
      "3627\n",
      "3657\n",
      "3693\n",
      "3728\n",
      "3765\n",
      "3797\n",
      "3823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3868\n",
      "3898\n",
      "3923\n",
      "3964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4005\n",
      "4044\n",
      "4077\n",
      "4116\n",
      "4148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4223\n",
      "4256\n",
      "4296\n",
      "4341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4375\n",
      "4409\n",
      "4438\n",
      "4475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4511\n",
      "4543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://terminatorsalvation.pizzahut.com.edgesuite.net/media/flvs/TerminatorETRL_High.flv TimeoutException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4661\n",
      "4691\n",
      "4725\n",
      "4763\n",
      "4802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4831\n",
      "4860\n",
      "4894\n",
      "4943\n",
      "4976\n",
      "5022\n",
      "5067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5104\n",
      "5137\n",
      "5177\n",
      "5215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5248\n",
      "http://dev.w3.org/html5/spec/Overview.html. TimeoutException\n",
      "http://www.whatwg.org/specs/web-apps/current-work/ TimeoutException\n",
      "5277\n",
      "http://stuffwhitepeoplelike.com/2008/11/18/116-black-music-that-black-people-dont-listen-to-anymore/ TimeoutException\n",
      "5301\n",
      "5333\n",
      "5376\n",
      "5414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5503\n",
      "5535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5577\n",
      "5609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5770\n",
      "5805\n",
      "5839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5879\n",
      "5922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5959\n",
      "5991\n"
     ]
    }
   ],
   "source": [
    "# make a basic training data set for bbc\n",
    "# url_keywords = ['youtube.com']\n",
    "\n",
    "# url_keywords = ['bbc.co.uk', 'cnn.com', 'wikipedia.org', 'www.youtube.com', 'www.imdb.com', 'www.washingtonpost.com']\n",
    "\n",
    "training_data = []\n",
    "\n",
    "for i, url in enumerate(urls_with_context):\n",
    "    \n",
    "    if i % 100 == 0: \n",
    "        # print(i / len(urls_with_context))\n",
    "        print(i, len(training_data))\n",
    "    \n",
    "    if len(training_data) == 6000:\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        signal.signal(signal.SIGALRM, handler)\n",
    "        signal.alarm(5) # 5 second timeout\n",
    "        \n",
    "        text = scrape(url['url'])\n",
    "        \n",
    "        signal.alarm(0) # disable alarm\n",
    "        \n",
    "        if text != '':\n",
    "            url['text'] = text\n",
    "            training_data.append(url)\n",
    "        # else:\n",
    "            # print(url['url'])\n",
    "    except Exception as e:\n",
    "        print(url['url'], e.__class__.__name__)\n",
    "\n",
    "#     for keyword in url_keywords:\n",
    "#         if keyword in url['url']:\n",
    "            \n",
    "# #             print(url['url'])\n",
    "\n",
    "#             try:\n",
    "#                 signal.signal(signal.SIGALRM, handler)\n",
    "#                 signal.alarm(5) # 5 second timeout\n",
    "                \n",
    "#                 text = scrape(url['url'])\n",
    "                \n",
    "#                 signal.alarm(0) # disable alarm\n",
    "                \n",
    "#                 if text != '':\n",
    "#                     url['text'] = text\n",
    "#                     training_data.append(url)\n",
    "#             except:\n",
    "#                 print(url['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets up a general training data set by adding context of all comments\n",
    "\n",
    "for example in training_data:\n",
    "    # get text of comment + ancestor comments\n",
    "    context = get_context(example)\n",
    "    example['full_context'] = context\n",
    "\n",
    "pickle.dump(training_data, open('../data/reddit/6000_scrape_raw.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format training data for pyserini (https://github.com/castorini/pyserini/)\n",
    "\n",
    "pyserini_retrieval_docs = []\n",
    "relevance_scores = []\n",
    "for i,example in enumerate(training_data):\n",
    "    doc = {\"id\": i, \"contents\": example['text']}\n",
    "    pyserini_retrieval_docs.append(doc)\n",
    "    relevance_score = str(i) + ' 0 ' + str(i) + ' 1'\n",
    "    relevance_scores.append(relevance_score)\n",
    "\n",
    "with open('../data/reddit/pyserini/bbc_news_pyserini.jsonl', 'w') as f:\n",
    "    for doc in pyserini_retrieval_docs:\n",
    "        f.write(json.dumps(doc) + '\\n')\n",
    "\n",
    "with open('../data/reddit/bbc_news_rel.txt', 'w') as f:\n",
    "    for rs in relevance_scores:\n",
    "        f.write(rs + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import _pickle as cPickle\n",
    "\n",
    "def compressed_pickle(title, data):\n",
    "    with bz2.BZ2File(title + '.pbz2', 'w') as f: \n",
    "        cPickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_pickle('../data/reddit/6000_scrape_raw', training_data)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "186f772927d269f45103655f755c1be5fcd73f402c4c2c68e610ffbef55b1b56"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('mjin': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import html2text\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Collect all comments by the post\n",
    "Only keep posts with more than 5 comments\n",
    "Takes about 3 minutes to run\n",
    "\n",
    "To get the reddit data (chosen arbitrarily), go to the data dir and run\n",
    "\n",
    "mkdir reddit\n",
    "cd reddit\n",
    "wget https://files.pushshift.io/reddit/comments/RC_2009-05.bz2\n",
    "bzip2 -d RC_2009-05.bz2\n",
    "'''\n",
    "\n",
    "comments_by_post = {}\n",
    "with open('../data/reddit/RC_2009-05', 'r') as f:\n",
    "    for line in f:\n",
    "        d = json.loads(line)\n",
    "        link_id = d['link_id']\n",
    "        if link_id not in comments_by_post:\n",
    "            comments_by_post[link_id] = {}\n",
    "        d['body'] = html2text.html2text(d['body'])\n",
    "        comments_by_post[link_id][d['id']] = d\n",
    "\n",
    "for key in list(comments_by_post.keys()):\n",
    "    if len(comments_by_post[key]) < 5:\n",
    "        del comments_by_post[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_ancestors(comments: dict, comment_id: str) -> list:\n",
    "    '''\n",
    "    For a given list of comments and a comment in the list, reconstruct a path to the top-level comment\n",
    "    Returns a list of comment IDs\n",
    "\n",
    "    This method isn't very efficient but good enough for now\n",
    "    '''\n",
    "    ancestors = []\n",
    "    while True:\n",
    "        \n",
    "        if comment_id[:2] == 't3': \n",
    "            # refers to a link (top-level comment)\n",
    "            # means we've reached the top of the chain\n",
    "            return ancestors[::-1]\n",
    "\n",
    "        if comment_id[:2] == 't1':\n",
    "            comment_id = comment_id[3:]\n",
    "\n",
    "\n",
    "        try:\n",
    "            # there is an error here sometimes where the comment id is not present in the list\n",
    "            # probably fine for now, but may need to address in the future\n",
    "            old_comment_id = comment_id\n",
    "            comment_id = comments[comment_id]['parent_id']\n",
    "            ancestors.append(old_comment_id)\n",
    "        except:\n",
    "            return ancestors[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Cycle through all posts and comments, find any URL mentions, and save the mention location + the comment's ancestors\n",
    "'''\n",
    "all_urls = []\n",
    "for post_id in comments_by_post:\n",
    "    for comment_id in comments_by_post[post_id]:\n",
    "        # first check if post body contains URL\n",
    "        urls = re.findall(r'(https?://\\S+)', comments_by_post[post_id][comment_id]['body'])\n",
    "        if urls != []:\n",
    "            ancestors = collect_ancestors(comments_by_post[post_id], comment_id)\n",
    "            for url in urls:\n",
    "                # heuristics for parsing errors\n",
    "                url = re.sub('\\)', '', url)\n",
    "                url = re.sub('\\]', '', url)\n",
    "                \n",
    "                all_urls.append({'post_id': post_id, 'comment_id': comment_id, 'url': url, 'ancestors': ancestors})\n",
    "\n",
    "urls_with_context = [x for x in all_urls if len(x['ancestors']) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loops through all ancestors of a URL comment and returns the chain up to to the top comment\n",
    "'''\n",
    "\n",
    "def display_context(url_obj: dict) -> list:\n",
    "    post_id = url_obj['post_id']\n",
    "    context = []\n",
    "    for ancestor in url_obj['ancestors']:\n",
    "        context.append(comments_by_post[post_id][ancestor]['body'])\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Simple method to scrape text from URLs. Not very robust. Need to handle exceptions. YouTube links take very long\n",
    "'''\n",
    "\n",
    "def scrape(url: str) -> str:\n",
    "    try:\n",
    "        html = urlopen(url).read()\n",
    "    except Exception as e:\n",
    "        return e\n",
    "\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"JKD Concepts would say he didn't do what's right. Ideally you want to\\nannihilate your opponent, and then run like hell, but that gets you into jail\\neven faster :(.\\n\\n\", \"What's JKD?\\n\\n\", '[Jeet Kun Do](http://en.wikipedia.org/wiki/Jeet_Kune_Do)\\n\\n']\n",
      "Jeet Kune Do - Wikipedia\n",
      "Jeet Kune Do\n",
      "From Wikipedia, the free encyclopedia\n",
      "Jump to navigation\n",
      "Jump to search\n",
      "Hybrid martial art\n",
      "This article has multiple issues. Please help improve it or discuss these issues on the talk page. (Learn how and when to remove these template messages)\n",
      "This article possibly contains original research. Please improve it by verifying the claims made and adding inline citations. Statements consisting only of original research should be removed. (March 2019) (Learn how and when to remove this template message)This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.Find sources: \"Jeet Kune Do\" – news · newspapers · books · scholar · JSTOR (August 2017) (Learn how and when to remove this template message)\n",
      "(Learn how and when to remove this template message)\n",
      "Jeet Kune Do截拳道The Jeet Kune Do Emblem\n",
      "The Taijitu represents the concepts of yin a\n"
     ]
    }
   ],
   "source": [
    "# test some basic outputs\n",
    "\n",
    "url_finding = urls_with_context[7]\n",
    "\n",
    "print(display_context(url_finding))\n",
    "print(scrape(url_finding['url'])[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.enamerique.com[(2(http://www.enamerique.net\n",
      "[('en.wikipedia.org', 6109), ('www.reddit.com', 3143), ('www.youtube.com', 2591), ('imgur.com', 524), ('www.google.com', 389), ('www.amazon.com', 342), ('www.imdb.com', 251), ('scriptures.lds.org', 217), ('news.bbc.co.uk', 169), ('www.biblegateway.com', 160), ('www.nytimes.com', 148), ('www.flickr.com', 137), ('tinyurl.com', 127), ('video.google.com', 119), ('www.guardian.co.uk', 106), ('dictionary.reference.com', 104), ('xkcd.com', 103), ('images.google.com', 92), ('upload.wikimedia.org', 81), ('lokonline.com', 76)]\n"
     ]
    }
   ],
   "source": [
    "# explore distribution of domains (help us with parsing)\n",
    "domains = {}\n",
    "for url in urls_with_context:\n",
    "    try:\n",
    "        domain = urlparse(url['url']).netloc\n",
    "    except:\n",
    "        print(url['url'])\n",
    "    if domain not in domains:\n",
    "        domains[domain] = 0\n",
    "    domains[domain] += 1\n",
    "\n",
    "sorted_domains = sorted([(domain, domains[domain]) for domain in domains], reverse=True, key=lambda x: x[1])\n",
    "print(sorted_domains[:20])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import html2text\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import pickle\n",
    "import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Collect all comments by the post\n",
    "Only keep posts with more than 5 comments\n",
    "Takes about 3 minutes to run on timan107\n",
    "\n",
    "To get the reddit data (chosen arbitrarily), go to the data dir and run\n",
    "\n",
    "mkdir reddit\n",
    "cd reddit\n",
    "wget https://files.pushshift.io/reddit/comments/RC_2009-05.bz2\n",
    "bzip2 -d RC_2009-05.bz2\n",
    "'''\n",
    "\n",
    "comments_by_post = {}\n",
    "with open('../data/reddit/RC_2009-05', 'r') as f:\n",
    "    for line in f:\n",
    "        d = json.loads(line)\n",
    "        link_id = d['link_id']\n",
    "        if link_id not in comments_by_post:\n",
    "            comments_by_post[link_id] = {}\n",
    "        d['body'] = html2text.html2text(d['body'])\n",
    "        comments_by_post[link_id][d['id']] = d\n",
    "        \n",
    "for key in list(comments_by_post.keys()):\n",
    "    if len(comments_by_post[key]) < 5:\n",
    "        del comments_by_post[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_ancestors(comments: dict, comment_id: str) -> list:\n",
    "    '''\n",
    "    For a given list of comments and a comment in the list, reconstruct a path to the top-level comment\n",
    "    Returns a list of comment IDs\n",
    "\n",
    "    This method isn't very efficient but good enough for now\n",
    "    '''\n",
    "    ancestors = []\n",
    "    while True:\n",
    "        \n",
    "        if comment_id[:2] == 't3': \n",
    "            # refers to a link (top-level comment)\n",
    "            # means we've reached the top of the chain\n",
    "            return ancestors[::-1]\n",
    "\n",
    "        if comment_id[:2] == 't1':\n",
    "            comment_id = comment_id[3:]\n",
    "\n",
    "        try:\n",
    "            # there is an error here sometimes where the comment id is not present in the list\n",
    "            # probably fine for now, but may need to address in the future\n",
    "            old_comment_id = comment_id\n",
    "            comment_id = comments[comment_id]['parent_id']\n",
    "            ancestors.append(old_comment_id)\n",
    "        except:\n",
    "            return ancestors[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Cycle through all posts and comments, find any URL mentions, and save the mention location + the comment's ancestors\n",
    "'''\n",
    "all_urls = []\n",
    "for post_id in comments_by_post:\n",
    "    for comment_id in comments_by_post[post_id]:\n",
    "        \n",
    "        # first check if post body contains URL\n",
    "#         urls = re.findall(r'(https?://\\S+)', comments_by_post[post_id][comment_id]['body'])\n",
    "#         len1 = len(urls)\n",
    "\n",
    "        # check if post body contains URL, accounts for edge case when dash is at the end of the line\n",
    "        current_comment_text = comments_by_post[post_id][comment_id]['body']\n",
    "        urls = re.findall(r'(https?://\\S+-\\n)?(?(1)([\\S]*)|(https?://\\S+))', current_comment_text)\n",
    "        \n",
    "#         len2 = len(urls)\n",
    "        \n",
    "        #if len1 != len2:\n",
    "        #    print(current_comment_text)\n",
    "                \n",
    "        if urls:\n",
    "            ancestors = collect_ancestors(comments_by_post[post_id], comment_id)\n",
    "            \n",
    "            for url in urls:\n",
    "                url = \"\".join(list(url))\n",
    "\n",
    "                # heuristics for parsing errors\n",
    "                url = re.sub('\\)', '', url)\n",
    "                url = re.sub('\\]', '', url)\n",
    "                url = re.sub('\\n', '', url)\n",
    "                \n",
    "                # remove non-alphnumeric characters\n",
    "                url_letters = re.sub('[^0-9a-zA-Z]', '', url)\n",
    "                                \n",
    "                # ignore pdfs\n",
    "                if 'pdf' == url_letters[-3:] or 'jpg' in url_letters[-3:] or 'png' in url_letters[-3:] or 'gif' in url_letters[-3:]:\n",
    "                    continue\n",
    "                \n",
    "                all_urls.append({'post_id': post_id, 'comment_id': comment_id, 'url': url, 'ancestors': ancestors})\n",
    "\n",
    "urls_with_context = [x for x in all_urls if len(x['ancestors']) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loops through all ancestors of a URL comment and returns the chain up to to the top comment\n",
    "'''\n",
    "\n",
    "def get_context(url_obj: dict) -> list:\n",
    "    post_id = url_obj['post_id']\n",
    "    context = []\n",
    "    for ancestor in url_obj['ancestors']:\n",
    "        context.append(comments_by_post[post_id][ancestor]['body'])\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Simple method to scrape text from URLs. Not very robust. Need to handle exceptions. YouTube links take very long\n",
    "'''\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def handler(signum, frame):\n",
    "    raise TimeoutException\n",
    "\n",
    "def scrape(url: str) -> str:\n",
    "    try:\n",
    "        html = urlopen(url).read()\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines and short lines\n",
    "    text = \"\"\n",
    "    for chunk in chunks:\n",
    "        if chunk and len(chunk) > 50:\n",
    "            # inspired by https://bigscience.huggingface.co/blog/building-a-tb-scale-multilingual-dataset-for-language-modeling#:~:text=Filters%2C%20tools%2C%20and%20indicators%20of%20data%20quality\n",
    "            only_text = re.sub('[^ \\w\\*]', '', chunk)\n",
    "            if len(only_text) / len(line) > 0.8:\n",
    "                text += chunk\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.enamerique.com[(2(http://www.enamerique.net\n",
      "[('en.wikipedia.org', 6012), ('www.reddit.com', 3141), ('www.youtube.com', 2591), ('www.google.com', 386), ('www.amazon.com', 340), ('www.imdb.com', 251), ('scriptures.lds.org', 217), ('news.bbc.co.uk', 166), ('www.biblegateway.com', 160), ('www.nytimes.com', 147), ('www.flickr.com', 137), ('tinyurl.com', 126), ('video.google.com', 119), ('www.guardian.co.uk', 106), ('dictionary.reference.com', 104), ('xkcd.com', 103), ('www.merriam-webster.com', 95), ('images.google.com', 92), ('lokonline.com', 76), ('lmgtfy.com', 75), ('books.google.com', 73), ('www.urbandictionary.com', 73), ('addons.mozilla.org', 64), ('maps.google.com', 56), ('www.cnn.com', 52), ('www.washingtonpost.com', 51), ('www.timesonline.co.uk', 50), ('www.msnbc.msn.com', 49), ('www.snopes.com', 48), ('pickyourhours.com', 48), ('www.theonion.com', 47), ('www.cdc.gov', 45), ('www.newegg.com', 44), ('www.yooouuutuuube.com', 44), ('code.google.com', 43), ('www.ted.com', 43), ('www.ncbi.nlm.nih.gov', 42), ('twitter.com', 42), ('www.hulu.com', 39), ('www.huffingtonpost.com', 39), ('www.salon.com', 39), ('en.wiktionary.org', 39), ('reddit.com', 37), ('mises.org', 36), ('userscripts.org', 36), ('memory-alpha.org', 36), ('www.npr.org', 35), ('web.archive.org', 34), ('thepiratebay.org', 34), ('scienceblogs.com', 34), ('1215.org', 33), ('www.telegraph.co.uk', 33), ('www.wired.com', 32), ('www.gnu.org', 32), ('www.myspace.com', 32), ('www.lewrockwell.com', 31), ('www.haaretz.com', 30), ('www.time.com', 30), ('www.slate.com', 30), ('translate.google.com', 30), ('www.independent.co.uk', 29), ('www.cirp.org', 29), ('www.talkorigins.org', 29), ('www.thefreedictionary.com', 28), ('answers.yahoo.com', 27), ('www.geocities.com', 27), ('www.nationmaster.com', 27), ('www.wolframalpha.com', 27), ('www.newscientist.com', 26), ('www.bbc.co.uk', 26), ('www.cbsnews.com', 26), ('www.straightdope.com', 26), ('imgur.com', 26), ('www.freethoughtpedia.com', 26), ('www.cia.gov', 25), ('www.instantrimshot.com', 25), ('instantrimshot.com', 25), ('plato.stanford.edu', 25), ('skepticsannotatedbible.com', 25), ('wordnetweb.princeton.edu', 24), ('wiki.answers.com', 24), ('www.boston.com', 24), ('tvtropes.org', 24), ('www.pbs.org', 24), ('www.dailymail.co.uk', 23), ('encyclopediadramatica.com', 23), ('online.wsj.com', 23), ('www.microsoft.com', 23), ('shootout.alioth.debian.org', 23), ('www.i4m.com', 23), ('www.law.cornell.edu', 22), ('digg.com', 22), ('www.foxnews.com', 22), ('www.reuters.com', 21), ('www.answers.com', 21), ('rationalwiki.com', 21), ('www.getdropbox.com', 21), ('www.usatoday.com', 21), ('code.reddit.com', 20), ('picasaweb.google.com', 20)]\n"
     ]
    }
   ],
   "source": [
    "# explore distribution of domains (help us with parsing)\n",
    "domains = {}\n",
    "for url in urls_with_context:\n",
    "    try:\n",
    "        domain = urlparse(url['url']).netloc\n",
    "    except:\n",
    "        print(url['url'])\n",
    "    if domain not in domains:\n",
    "        domains[domain] = 0\n",
    "    domains[domain] += 1\n",
    "\n",
    "sorted_domains = sorted([(domain, domains[domain]) for domain in domains], reverse=True, key=lambda x: x[1])\n",
    "print(sorted_domains[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "http://en.wikipedia.org/wiki/Koreans\n",
      "http://en.wikipedia.org/wiki/Lloyd_Doggett#Abortion\n",
      "http://en.wikipedia.org/wiki/Excommunication#Jehovah.27s_Witnesses\n",
      "http://en.wikipedia.org/wiki/BBC#Revenue\n",
      "http://en.wikipedia.org/wiki/Arab_citizens_of_Israel\n",
      "http://en.wikipedia.org/wiki/Amnesty_International#Work.\n",
      "http://en.wikipedia.org/wiki/Bush_v._Gore\n",
      "http://en.wikipedia.org/wiki/List_of_common_misconceptions#Islam\n",
      "http://en.wikipedia.org/wiki/Israel#Religion\n",
      "0.0323907621546335\n",
      "http://en.wikipedia.org/wiki/Copyright#History\n",
      "http://en.wikipedia.org/wiki/List_of_countries_by_suicide_rate\n",
      "http://en.wikipedia.org/wiki/French_Republic\n",
      "http://en.wikipedia.org/wiki/Africa#Colonialism_and_the_.22Scramble_for_Africa.22\n",
      "http://en.wikipedia.org/wiki/Crime_in_the_United_States#Characteristics_of_offenders\n",
      "http://en.wikipedia.org/wiki/Peckham_Library\n",
      "0.064781524309267\n",
      "http://en.wikipedia.org/wiki/Bladder_cancer\n",
      "http://news.bbc.co.uk/1/hi/uk/3996677.stm\n",
      "http://en.wikipedia.org/wiki/France\n",
      "0.0971722864639005\n",
      "http://en.wikipedia.org/wiki/George_Bush_Jr\n",
      "http://en.wikipedia.org/wiki/Enron\n",
      "0.129563048618534\n",
      "http://en.wikipedia.org/wiki/Slavery_in_the_United_States\n",
      "http://en.wikipedia.org/wiki/Operation_Condor\n",
      "http://en.wikipedia.org/wiki/Bay_of_Pigs_Invasion\n",
      "http://en.wikipedia.org/wiki/Pearl_Harbor_advance-knowledge_debate\n",
      "http://en.wikipedia.org/wiki/Google_Android\n",
      "0.1619538107731675\n",
      "http://en.wikipedia.org/wiki/MMR_vaccine_controversy#Disease_outbreaks\n",
      "http://en.wikipedia.org/wiki/Legality_of_cannabis\n",
      "http://en.wikipedia.org/wiki/Microwave_power_transmission\n",
      "http://en.wikipedia.org/wiki/Radio_waves\n",
      "http://en.wikipedia.org/wiki/Rapid_transit\n",
      "0.194344572927801\n",
      "http://en.wikipedia.org/wiki/Vichy_France\n",
      "http://news.bbc.co.uk/2/hi/middle_east/7047133.stm\n",
      "http://en.wikipedia.org/wiki/Hippocampus\n",
      "0.22673533508243449\n",
      "http://en.wikipedia.org/wiki/Napoleon_I_of_France#Image,\n",
      "http://en.wikipedia.org/wiki/Faith#Christianity\n",
      "http://en.wikipedia.org/wiki/David_Attenborough\n",
      "http://en.wikipedia.org/wiki/Vaccine\n",
      "http://en.wikipedia.org/wiki/Shakespeare's_influence\n",
      "0.259126097237068\n",
      "http://en.wikipedia.org/wiki/SS\n",
      "http://en.wikipedia.org/wiki/Nanotechnology\n",
      "http://en.wikipedia.org/wiki/Windows_Presentation_Foundation\n",
      "http://en.wikipedia.org/wiki/Firefly_(TV_series%29\n",
      "http://en.wikipedia.org/wiki/Histrionic_personality_disorder\n",
      "http://en.wikipedia.org/wiki/Michael_Scheuer\n",
      "http://en.wikipedia.org/wiki/Energy_density\n",
      "http://en.wikipedia.org/wiki/Communism\n",
      "http://en.wikipedia.org/wiki/Libertarian_socialism#Political_roots\n",
      "http://en.wikipedia.org/wiki/The_Troubles\n",
      "http://en.wikipedia.org/wiki/Arnold_Schwartzeneggar\n",
      "http://en.wikipedia.org/wiki/World_War_II_casualties\n",
      "http://en.wikipedia.org/wiki/Hamas#Goals\n",
      "http://en.wikipedia.org/wiki/Uncertainty_principle\n",
      "http://en.wikipedia.org/wiki/Golden_ratio#Relationship_to_Fibonacci_sequence\n",
      "0.29151685939170147\n",
      "http://en.wikipedia.org/wiki/Wikipedia:MATH\n",
      "http://en.wikipedia.org/wiki/The_Beatles\n",
      "http://en.wikipedia.org/wiki/William_Henry_Harrison\n",
      "0.323907621546335\n",
      "http://en.wikipedia.org/wiki/Southern_Baptist_Convention\n",
      "http://en.wikipedia.org/wiki/American_Baptist_Churches_USA\n",
      "http://en.wikipedia.org/wiki/Anne_Rice#Fan_fiction\n",
      "http://en.wikipedia.org/wiki/Taliban#Life_under_the_Taliban_regime.\n",
      "0.3562983837009685\n",
      "http://en.wikipedia.org/wiki/Virtual_retinal_display\n",
      "http://en.wikipedia.org/wiki/Europe#Definition(http://en.wikipedia.org/wiki/Europe#Definition\n",
      "http://en.wikipedia.org/wiki/Lebanon\n",
      "http://en.wikipedia.org/wiki/United_States_Senate_elections,_2010\n",
      "0.388689145855602\n",
      "http://en.wikipedia.org/wiki/Denazification#Collective_guilt_campaign,\n",
      "http://en.wikipedia.org/wiki/Peter_Singer#Abortion.2C_euthanasia_and_infanticide\n",
      "http://en.wikipedia.org/wiki/Gender#The_word_gender_in_English\n",
      "http://en.wikipedia.org/wiki/Irish_famine#Causes_and_contributing_factors\n",
      "0.4210799080102355\n",
      "http://en.wikipedia.org/wiki/Pope_Benedict_XVI#Early_life:_1927.E2.80.931951\n",
      "http://en.wikipedia.org/wiki/Apollo_11#Launch_and_lunar_landing\n",
      "0.45347067016486897\n",
      "http://en.wikipedia.org/wiki/Jainism\n",
      "http://en.wikipedia.org/wiki/Life#Definitions\n",
      "http://en.wikipedia.org/wiki/Wikipedia:Unusual_articles\n",
      "http://en.wikipedia.org/wiki/Game_theory\n",
      "0.48586143231950246\n",
      "http://en.wikipedia.org/wiki/Jesus#Mythical_view\n",
      "http://en.wikipedia.org/wiki/Athena\n",
      "http://en.wikipedia.org/wiki/Alexander_Graham_Bell\n",
      "http://en.wikipedia.org/wiki/Maxwell's_equations\n",
      "http://en.wikipedia.org/wiki/Libertarian_socialism\n"
     ]
    }
   ],
   "source": [
    "# make a basic training data set for bbc\n",
    "# url_keywords = ['youtube.com']\n",
    "url_keywords = ['bbc.co.uk', 'cnn.com', 'wikipedia.org']\n",
    "training_data = []\n",
    "\n",
    "for i, url in enumerate(urls_with_context):\n",
    "    if i % 1000 == 0: print(i / len(urls_with_context))\n",
    "    \n",
    "    if len(training_data) == 2000:\n",
    "        break\n",
    "\n",
    "    for keyword in url_keywords:\n",
    "        if keyword in url['url']:\n",
    "            \n",
    "#             print(url['url'])\n",
    "\n",
    "            try:\n",
    "                signal.signal(signal.SIGALRM, handler)\n",
    "                signal.alarm(1) # 5 second timeout\n",
    "                \n",
    "                text = scrape(url['url'])\n",
    "                \n",
    "                signal.alarm(0) # disable alarm\n",
    "                \n",
    "                if text != '':\n",
    "                    url['text'] = text\n",
    "                    training_data.append(url)\n",
    "            except:\n",
    "                print(url['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets up a general training data set by adding context of all comments\n",
    "\n",
    "for example in training_data:\n",
    "    # get text of comment + ancestor comments\n",
    "    context = get_context(example)\n",
    "    example['full_context'] = context\n",
    "\n",
    "pickle.dump(training_data, open('../data/reddit/bbc_news_scrape_raw.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for example in training_data:\n",
    "    print(example['text'], 2 * \"\\n\", example['full_context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format training data for pyserini (https://github.com/castorini/pyserini/)\n",
    "\n",
    "pyserini_retrieval_docs = []\n",
    "relevance_scores = []\n",
    "for i,example in enumerate(training_data):\n",
    "    doc = {\"id\": i, \"contents\": example['text']}\n",
    "    pyserini_retrieval_docs.append(doc)\n",
    "    relevance_score = str(i) + ' 0 ' + str(i) + ' 1'\n",
    "    relevance_scores.append(relevance_score)\n",
    "\n",
    "with open('../data/reddit/pyserini/bbc_news_pyserini.jsonl', 'w') as f:\n",
    "    for doc in pyserini_retrieval_docs:\n",
    "        f.write(json.dumps(doc) + '\\n')\n",
    "\n",
    "with open('../data/reddit/bbc_news_rel.txt', 'w') as f:\n",
    "    for rs in relevance_scores:\n",
    "        f.write(rs + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

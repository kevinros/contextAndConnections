{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import signal\n",
    "from urllib.parse import urlparse\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train = pickle.load(open('../data/encoded_queries_train.pkl', 'rb'))\n",
    "src_dev = pickle.load(open('../data/encoded_queries_dev.pkl', 'rb'))\n",
    "\n",
    "raw_websites = json.load(open('../data/RC_2009-05_webpages.json', 'r'))\n",
    "\n",
    "\n",
    "corpus_text = []\n",
    "with open('../data/websites/bow_websites.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        corpus_text.append(json.loads(line))\n",
    "\n",
    "src_dev_text = open('../data/bow_queries_dev.tsv', 'r').read().splitlines()\n",
    "\n",
    "\n",
    "corpus = pickle.load(open('../data/encoded_websites.pkl', 'rb'))\n",
    "\n",
    "\n",
    "relevance_scores = open('../data/relevance_scores.txt', 'r')\n",
    "query_webpage_map = {}\n",
    "for line in relevance_scores:\n",
    "    split_line = line.split()\n",
    "    query_webpage_map[int(split_line[0])] = int(split_line[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author': 'JGMouton4246', 'author_flair_css_class': None, 'author_flair_text': None, 'body': 'Hi, I been a fan of Milovana for a while now, pls send me an invite code in PM. Thanks ', 'can_gild': True, 'collapsed': False, 'collapsed_reason': None, 'controversiality': 0, 'created_utc': 1504224000, 'distinguished': None, 'edited': False, 'gilded': 0, 'id': 'dmehp0v', 'is_submitter': False, 'link_id': 't3_6kkdkq', 'parent_id': 't3_6kkdkq', 'retrieved_on': 1504556642, 'score': 1, 'stickied': False, 'subreddit': 'Milovana', 'subreddit_id': 't5_31hip'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data_2017-09/RC_2017-09\", 'r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        comment = json.loads(line)\n",
    "        print(comment)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient way of creating training data from large comment JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid URLs collected:  10000\n",
      "Valid URLs collected:  20000\n",
      "Valid URLs collected:  30000\n",
      "Valid URLs collected:  40000\n",
      "Valid URLs collected:  50000\n",
      "Valid URLs collected:  60000\n",
      "Valid URLs collected:  70000\n",
      "Valid URLs collected:  80000\n",
      "Valid URLs collected:  90000\n",
      "Valid URLs collected:  100000\n",
      "Valid URLs collected:  110000\n",
      "Valid URLs collected:  120000\n",
      "Valid URLs collected:  130000\n",
      "Valid URLs collected:  140000\n",
      "Valid URLs collected:  150000\n",
      "Valid URLs collected:  160000\n",
      "Valid URLs collected:  170000\n",
      "Valid URLs collected:  180000\n",
      "Valid URLs collected:  190000\n",
      "Valid URLs collected:  200000\n",
      "Valid URLs collected:  210000\n",
      "Valid URLs collected:  220000\n",
      "Valid URLs collected:  230000\n",
      "Valid URLs collected:  240000\n",
      "Valid URLs collected:  250000\n",
      "All valid URLs collected\n",
      "All id chains constructed\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Notes\n",
    "1. There are 83,165,192 comments --> approx 250,000 links in comments\n",
    "2. Total pass-through ~25 minutes, and we do two pass-throughs\n",
    "3. en.m.wiki is mapped to en.wiki\n",
    "'''\n",
    "\n",
    "valid_domains = {'en.wikipedia.org': True, 'en.m.wikipedia.org': True, 'www.washingtonpost.com': True, 'www.theguardian.com': True, 'www.independent.co.uk': True,\n",
    "          'www.theatlantic.com': True, 'www.bbc.com': True, 'www.nbcnews.com': True, 'www.usatoday.com': True, 'www.cnn.com': True, 'insider.foxnews.com': True,\n",
    "          'www.npr.org': True, 'www.espn.com': True, 'www.politico.com': True, 'www.bbc.co.uk': True, 'www.telegraph.co.uk': True, 'www.businessinsider.com': True,\n",
    "          'www.bloomberg.com': True, 'www.bbc.co.uk': True, 'www.forbes.com': True, 'abcnews.go.com': True, 'www.huffingtonpost.com': True, 'www.latimes.com': True,\n",
    "          'www.pbs.org': True, 'www.thesun.co.uk': True, 'www.chicagotribune.com': True, 'www.dailymail.co.uk': True, 'www.cnbc.com': True, 'www.foxnews.com': True,\n",
    "          'www.slate.com': True, 'www.wired.com': True, 'www.investopedia.com': True, 'www.theonion.com': True, 'www.vox.com': True, 'articles.chicagotribune.com': True}\n",
    "\n",
    "ignore_type = {'pdf': True, 'jpg': True, 'png':True, 'gif':True}\n",
    "\n",
    "all_urls = {}\n",
    "valid_urls = {}\n",
    "id_parent_mapping = {}\n",
    "\n",
    "DATA_PATH = \"../data_2017-09/RC_2017-09\"\n",
    "\n",
    "with open(DATA_PATH, 'r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        comment = json.loads(line)\n",
    "\n",
    "\n",
    "        id_parent_mapping[comment['id']] = comment['parent_id'][3:]\n",
    "\n",
    "        cleaned_urls = []\n",
    "        cleaned_valid_urls = []\n",
    "        for url in re.findall(r'(https?://\\S+-\\n)?(?(1)([\\S]*)|(https?://\\S+))', comment['body']):\n",
    "\n",
    "            # heuristics for parsing url\n",
    "            url = \"\".join(url)\n",
    "            url = re.sub('\\)', '', url)\n",
    "            url = re.sub('\\]', '', url)\n",
    "            url = re.sub('\\n', '', url)\n",
    "            url = re.sub(',', '', url)\n",
    "            url = re.sub(' ', '', url)\n",
    "            if url[-1] == \".\":\n",
    "                url = url[:-1]\n",
    "\n",
    "            try:\n",
    "                domain = urlparse(url).netloc\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            if domain == \"en.m.wikipedia.org\":\n",
    "                url = re.sub('\\.m', '', url)\n",
    "\n",
    "            cleaned_urls.append(url)\n",
    "\n",
    "            if url[-3:] in ignore_type: continue\n",
    "            \n",
    "            if domain not in valid_domains: continue\n",
    "            \n",
    "            cleaned_valid_urls.append(url)\n",
    "\n",
    "        if cleaned_urls:\n",
    "            all_urls[i] = {'urls': cleaned_urls}\n",
    "        if cleaned_valid_urls:\n",
    "            valid_urls[i] = {'urls': cleaned_valid_urls, 'id': comment['id']}\n",
    "            if len(valid_urls) % 10000 == 0: print('Valid URLs collected: ', len(valid_urls))\n",
    "\n",
    "print('All valid URLs collected')\n",
    "\n",
    "# build the chains for each valid URL\n",
    "all_needed_comments = {}\n",
    "for line in valid_urls:\n",
    "    id = valid_urls[line]['id']\n",
    "    chain = []\n",
    "    while id in id_parent_mapping:\n",
    "        chain.append(id)\n",
    "        all_needed_comments[id] = None\n",
    "        id = id_parent_mapping[id]\n",
    "    chain = chain[::-1]\n",
    "    valid_urls[line]['chain'] = chain\n",
    "\n",
    "print('All id chains constructed')\n",
    "\n",
    "# do a second pass-through to get all necessary comments\n",
    "with open(DATA_PATH, 'r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        comment = json.loads(line)\n",
    "        comment_id = comment['id']\n",
    "        if comment_id in all_needed_comments:\n",
    "            all_needed_comments[comment_id] = comment['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all_needed_comments pkl\n",
    "# save valid_urls pkl\n",
    "# save all_urls pkl\n",
    "pickle.dump(all_needed_comments, open('../data_2017-09/all_needed_comments.pkl', 'wb'))\n",
    "pickle.dump(valid_urls, open('../data_2017-09/valid_urls.pkl', 'wb'))\n",
    "pickle.dump(all_urls, open('../data_2017-09/all_urls.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient way to scrape and save websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total webpages to fetch:  130193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def handler(signum, frame):\n",
    "    raise TimeoutException\n",
    "\n",
    "def scrape(url: str) -> str:\n",
    "    '''\n",
    "    Simple method to scrape text from URLs. Not very robust. Need to handle exceptions. YouTube links take very long\n",
    "    '''\n",
    "    try:\n",
    "        html = urlopen(url).read()\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines and short lines\n",
    "    text = \"\"\n",
    "    for chunk in chunks:\n",
    "        if chunk and len(chunk) > 50:\n",
    "            # inspired by https://bigscience.huggingface.co/blog/building-a-tb-scale-multilingual-dataset-for-language-modeling#:~:text=Filters%2C%20tools%2C%20and%20indicators%20of%20data%20quality\n",
    "            only_text = re.sub('[^ \\w\\*]', '', chunk)\n",
    "            if len(only_text) / len(chunk) > 0.8:\n",
    "                text += chunk\n",
    "    return text\n",
    "\n",
    "\n",
    "# Conditions\n",
    "MIN_LENGTH=2\n",
    "\n",
    "url_file_map = {}\n",
    "for line in valid_urls:\n",
    "    if len(valid_urls[line]['chain']) < MIN_LENGTH:\n",
    "        continue\n",
    "    for url in valid_urls[line]['urls']:\n",
    "        url_file_map[url] = {'filename': uuid.uuid4().hex}\n",
    "\n",
    "pickle.dump(url_file_map, open('../data_2017-09/url_file_map.pkl', 'wb'))\n",
    "\n",
    "total_urls = len(url_file_map)\n",
    "print('Total webpages to fetch: ', total_urls)\n",
    "for i,url in enumerate(url_file_map):\n",
    "    url_file_map[url]['status'] = 'failure'\n",
    "\n",
    "    signal.signal(signal.SIGALRM, handler)\n",
    "    signal.alarm(2) # 5 second timeout\n",
    "    try:\n",
    "        text = scrape(url)\n",
    "    except:\n",
    "        continue\n",
    "    if len(text.strip()) > 100:\n",
    "        with open('../data_2017-09/webpages/' + url_file_map[url]['filename'] + '.txt', 'w') as f:\n",
    "            f.write(text)\n",
    "            url_file_map[url]['status'] = 'success'\n",
    "        \n",
    "    signal.alarm(0) # disable alarm\n",
    "    if i > 100: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain_freqs(urls):\n",
    "    domains = {}\n",
    "    for url in urls:\n",
    "        try:\n",
    "            domain = urlparse(urls[url]['urls'][0]).netloc\n",
    "        except:\n",
    "            print(urls[url]['urls'][0])\n",
    "        if domain not in domains:\n",
    "            domains[domain] = 0\n",
    "        domains[domain] += 1\n",
    "\n",
    "    sorted_domains = sorted([(domain, domains[domain]) for domain in domains], reverse=True, key=lambda x: x[1])\n",
    "    for d in sorted_domains[:1000]:\n",
    "        #if d[0] in valid_domains: continue\n",
    "        print(d)\n",
    "\n",
    "get_domain_freqs(valid_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_id': 5335,\n",
       " 'encoding': tensor([[ 0.0080,  0.0120, -0.0139,  ..., -0.0024,  0.0485, -0.0249],\n",
       "         [ 0.0336,  0.0337,  0.0075,  ...,  0.0383,  0.0492,  0.0096]],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_dev[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5335\\twhat a world it would be without Murdock. [Murdock]( )[Murdoch](http://en.wikipedia.org/wiki/Rupert_Murdoch)'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_dev_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_webpage_map[5335]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of characters from the American TV show The A-TeamThis article lists the characters created for\n"
     ]
    }
   ],
   "source": [
    "print(corpus_text[200]['contents'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5953\n",
      "5953\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))\n",
    "print(len(corpus_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "British weekly news and international affairs publicationFor the profession, see Economist. For othe\n",
      "======\n",
      "John R. Mashey (born 1946) is an American computer scientist, director and entrepreneur.Mashey holds\n",
      "======\n",
      "For a non-technical introduction to the topic, see Introduction to genetics. For other uses, see DNA\n",
      "======\n",
      "System of interlinked hypertext documents accessed over the Internet\"WWW\" and \"The Web\" redirect her\n",
      "======\n",
      "This article includes a list of general references, but it lacks sufficient corresponding inline cit\n",
      "======\n",
      "Nonprofit news organization owned by the Church of Christ, ScientistThe Christian Science MonitorThe\n",
      "======\n",
      "Magazine and multi-platform publisher based in Washington, D.C.For the ocean, see Atlantic Ocean. Fo\n",
      "======\n",
      "Type and severity of damage caused by nuclear weaponsThis article includes a list of general referen\n",
      "======\n",
      "This article is about the British newspaper. For the Australian newspaper, see The Daily Telegraph (\n",
      "======\n",
      "Alain de BottonFRSLBorn (1969-12-20) 20 December 1969 (age 52)Zürich, SwitzerlandOccupationWriter, S\n",
      "======\n",
      "Online multiplayer gaming and digital media delivery service by MicrosoftXbox networkDeveloperMicros\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "for c in corpus_text:\n",
    "    web_text = c['contents']\n",
    "    if \"paywall\" in web_text: \n",
    "        print(web_text[:100])\n",
    "        print('======')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

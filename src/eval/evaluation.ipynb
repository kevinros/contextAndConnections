{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change for whichever run you're evaluating\n",
    "path_to_run = 'out/bm25rm3_runs/v2_run.test_8_0.99_0.9_1_10.txt'\n",
    "path_to_relevance_scores = 'data_2017-09/queries/relevance_scores.txt'\n",
    "\n",
    "# This is important, so that we know how many queries to expect. Can use the full setting in all cases, just need to change between train/dev/test. \n",
    "all_query_path = 'data_2017-09/queries/queries_test.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = eval.load_run(path_to_run)\n",
    "rs = eval.load_rel_scores(path_to_relevance_scores)\n",
    "q_ids = eval.load_query_ids(all_query_path)\n",
    "print('P@1: ', eval.compute_p1(rs,run, q_ids))\n",
    "print('MRR@10: ', eval.compute_mrr10(rs,run, q_ids))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
